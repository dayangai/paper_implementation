{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2563,
     "status": "ok",
     "timestamp": 1622730528018,
     "user": {
      "displayName": "Yizhou Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKweum2fN1Zce6MyyPbghO4Je2ulM6lfRBIfn7=s64",
      "userId": "03363274278730990809"
     },
     "user_tz": -120
    },
    "id": "3JtolaE6Y5h-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Get number of training examples\n",
    "        N = query.shape[0]\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
    "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
    "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
    "\n",
    "        # Einsum does matrix mult. for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just how I like doing matrix multiplication & bmm\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, heads_dim),\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # energy: (N, heads, query_len, key_len)\n",
    "\n",
    "        # Mask padded indices so their weights become 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for\n",
    "        # better stability\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
    "        # we reshape and flatten the last two dimensions.\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        # Linear layer doesn't modify the shape, final shape will be\n",
    "        # (N, query_len, embed_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1622730528019,
     "user": {
      "displayName": "Yizhou Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKweum2fN1Zce6MyyPbghO4Je2ulM6lfRBIfn7=s64",
      "userId": "03363274278730990809"
     },
     "user_tz": -120
    },
    "id": "oLqmvu9yY5h_"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1622730528019,
     "user": {
      "displayName": "Yizhou Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKweum2fN1Zce6MyyPbghO4Je2ulM6lfRBIfn7=s64",
      "userId": "03363274278730990809"
     },
     "user_tz": -120
    },
    "id": "gjEidK56Y5h_"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        out = self.dropout(\n",
    "            (self.word_embedding(x) + self.position_embedding(positions))\n",
    "        )\n",
    "\n",
    "        # In the Encoder the query, key, value are all the same, it's in the\n",
    "        # decoder this will change. This might look a bit odd in this case.\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1622730528434,
     "user": {
      "displayName": "Yizhou Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKweum2fN1Zce6MyyPbghO4Je2ulM6lfRBIfn7=s64",
      "userId": "03363274278730990809"
     },
     "user_tz": -120
    },
    "id": "wY61U9RNY5iA"
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = SelfAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1622730529246,
     "user": {
      "displayName": "Yizhou Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKweum2fN1Zce6MyyPbghO4Je2ulM6lfRBIfn7=s64",
      "userId": "03363274278730990809"
     },
     "user_tz": -120
    },
    "id": "U3gGj-NQY5iA"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1622730530649,
     "user": {
      "displayName": "Yizhou Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKweum2fN1Zce6MyyPbghO4Je2ulM6lfRBIfn7=s64",
      "userId": "03363274278730990809"
     },
     "user_tz": -120
    },
    "id": "NxG_7IpiY5iB"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_size,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        heads,\n",
    "        num_layers,\n",
    "        num_layer,#\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length=100,\n",
    "        \n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = 1\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # (N, 1, 1, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38079,
     "status": "ok",
     "timestamp": 1622730572468,
     "user": {
      "displayName": "Yizhou Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKweum2fN1Zce6MyyPbghO4Je2ulM6lfRBIfn7=s64",
      "userId": "03363274278730990809"
     },
     "user_tz": -120
    },
    "id": "TABvRd3RY5iD",
    "outputId": "932f0058-8eb4-4c38-cf0a-36ea7b537398"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy==3.0.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/d8/0361bbaf7a1ff56b44dca04dace54c82d63dad7475b7d25ea1baefafafb2/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8MB 2.9MB/s \n",
      "\u001b[?25hCollecting pathy>=0.3.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/87/5991d87be8ed60beb172b4062dbafef18b32fa559635a8e2b633c2974f85/pathy-0.5.2-py3-none-any.whl (42kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (1.19.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (2.23.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (4.41.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (20.9)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (3.7.4.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (3.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (2.0.5)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 51.3MB/s \n",
      "\u001b[?25hCollecting typer<0.4.0,>=0.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (1.0.5)\n",
      "Collecting pydantic<1.8.0,>=1.7.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/fa/d43f31874e1f2a9633e4c025be310f2ce7a8350017579e9e837a62630a7e/pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
      "\u001b[K     |████████████████████████████████| 9.1MB 24.0MB/s \n",
      "\u001b[?25hCollecting thinc<8.1.0,>=8.0.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/87/decceba68a0c6ca356ddcb6aea8b2500e71d9bc187f148aae19b747b7d3c/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 53.5MB/s \n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.4\n",
      "  Downloading https://files.pythonhosted.org/packages/8d/67/d4002a18e26bf29b17ab563ddb55232b445ab6a02f97bf17d1345ff34d3f/spacy_legacy-3.0.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (2.11.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (57.0.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/9c/10/dbc1203a4b1367c7b02fddf08cb2981d9aa3e688d398f587cea0ab9e3bec/catalogue-2.0.4-py3-none-any.whl\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.0.6) (0.8.2)\n",
      "Collecting smart-open<4.0.0,>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 56.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.6) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.6) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.6) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.6) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.0.6) (2.4.7)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy==3.0.6) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.0.6) (2.0.1)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy==3.0.6) (3.4.1)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107107 sha256=af837c16c37f2fe25c9715d57719a767fc4048f7c536d06d9209bf79950f4973\n",
      "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, typer, pathy, catalogue, srsly, pydantic, thinc, spacy-legacy, spacy\n",
      "  Found existing installation: smart-open 5.0.0\n",
      "    Uninstalling smart-open-5.0.0:\n",
      "      Successfully uninstalled smart-open-5.0.0\n",
      "  Found existing installation: catalogue 1.0.0\n",
      "    Uninstalling catalogue-1.0.0:\n",
      "      Successfully uninstalled catalogue-1.0.0\n",
      "  Found existing installation: srsly 1.0.5\n",
      "    Uninstalling srsly-1.0.5:\n",
      "      Successfully uninstalled srsly-1.0.5\n",
      "  Found existing installation: thinc 7.4.0\n",
      "    Uninstalling thinc-7.4.0:\n",
      "      Successfully uninstalled thinc-7.4.0\n",
      "  Found existing installation: spacy 2.2.4\n",
      "    Uninstalling spacy-2.2.4:\n",
      "      Successfully uninstalled spacy-2.2.4\n",
      "Successfully installed catalogue-2.0.4 pathy-0.5.2 pydantic-1.7.4 smart-open-3.0.0 spacy-3.0.6 spacy-legacy-3.0.5 srsly-2.4.1 thinc-8.0.3 typer-0.3.2\n",
      "Collecting torchtext==0.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 2.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 4.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.6.0) (1.8.1+cu101)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "  Found existing installation: torchtext 0.9.1\n",
      "    Uninstalling torchtext-0.9.1:\n",
      "      Successfully uninstalled torchtext-0.9.1\n",
      "Successfully installed sentencepiece-0.1.95 torchtext-0.6.0\n",
      "2021-06-03 14:29:16.499811: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Collecting en-core-web-sm==3.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7MB 449kB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (57.0.0)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Found existing installation: en-core-web-sm 2.2.5\n",
      "    Uninstalling en-core-web-sm-2.2.5:\n",
      "      Successfully uninstalled en-core-web-sm-2.2.5\n",
      "Successfully installed en-core-web-sm-3.0.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "2021-06-03 14:29:24.529764: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Collecting de-core-news-sm==3.0.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.0.0/de_core_news_sm-3.0.0-py3-none-any.whl (19.3MB)\n",
      "\u001b[K     |████████████████████████████████| 19.3MB 1.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.7.4)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (57.0.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.23.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (4.41.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.7.4.3)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.4.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->de-core-news-sm==3.0.0) (2.0.1)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.0.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install spacy==3.0.6\n",
    "!sudo pip install torchtext==0.6.0\n",
    "!sudo python -m spacy download en_core_web_sm\n",
    "!sudo python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 4041,
     "status": "ok",
     "timestamp": 1622730576507,
     "user": {
      "displayName": "Yizhou Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKweum2fN1Zce6MyyPbghO4Je2ulM6lfRBIfn7=s64",
      "userId": "03363274278730990809"
     },
     "user_tz": -120
    },
    "id": "QcDRAlU8Y5iD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import spacy\n",
    "from utils_scratch import translate_sentence, bleu, save_checkpoint, load_checkpoint\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "\"\"\"\n",
    "To install spacy languages do:\n",
    "python -m spacy download en\n",
    "python -m spacy download de\n",
    "\"\"\"\n",
    "spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1622730576509,
     "user": {
      "displayName": "Yizhou Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKweum2fN1Zce6MyyPbghO4Je2ulM6lfRBIfn7=s64",
      "userId": "03363274278730990809"
     },
     "user_tz": -120
    },
    "id": "ylnV8qnRY5iD"
   },
   "outputs": [],
   "source": [
    "def tokenize_ger(text):\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "def tokenize_eng(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1622730576510,
     "user": {
      "displayName": "Yizhou Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKweum2fN1Zce6MyyPbghO4Je2ulM6lfRBIfn7=s64",
      "userId": "03363274278730990809"
     },
     "user_tz": -120
    },
    "id": "DV9GHLWVY5iD"
   },
   "outputs": [],
   "source": [
    "german=Field(tokenize=tokenize_ger,lower=True,init_token=\"<sos>\",eos_token=\"<eos>\")\n",
    "english=Field(tokenize=tokenize_eng,lower=True,init_token=\"<sos>\",eos_token=\"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11725,
     "status": "ok",
     "timestamp": 1622730588231,
     "user": {
      "displayName": "Yizhou Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKweum2fN1Zce6MyyPbghO4Je2ulM6lfRBIfn7=s64",
      "userId": "03363274278730990809"
     },
     "user_tz": -120
    },
    "id": "H1eCR4HFY5iD",
    "outputId": "d7250294-ebc8-4eb3-8b90-fb1be2b87069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading training.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:02<00:00, 603kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading validation.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 92.3kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading mmt_task1_test2016.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 88.3kB/s]\n"
     ]
    }
   ],
   "source": [
    "train_data,valid_data,test_data=Multi30k.splits(\n",
    "    exts=(\".de\",\".en\"),fields=(german,english)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1622730588231,
     "user": {
      "displayName": "Yizhou Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKweum2fN1Zce6MyyPbghO4Je2ulM6lfRBIfn7=s64",
      "userId": "03363274278730990809"
     },
     "user_tz": -120
    },
    "id": "x6-II67AY5iD"
   },
   "outputs": [],
   "source": [
    "german.build_vocab(train_data,max_size=10000,min_freq=2)\n",
    "english.build_vocab(train_data,max_size=10000,min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1622730588232,
     "user": {
      "displayName": "Yizhou Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKweum2fN1Zce6MyyPbghO4Je2ulM6lfRBIfn7=s64",
      "userId": "03363274278730990809"
     },
     "user_tz": -120
    },
    "id": "GWc-b7hS4-Ji"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
    "    # Load german tokenizer\n",
    "    spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "    if type(sentence) == str:\n",
    "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # Add <SOS> and <EOS> in beginning and end respectively\n",
    "    tokens.insert(0, german.init_token)\n",
    "    tokens.append(german.eos_token)\n",
    "\n",
    "    # Go through each german token and convert to an index\n",
    "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
    "    for i in range(max_length):\n",
    "        trg_tensor = torch.LongTensor(outputs).unsqueeze(0).to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            output = model(sentence_tensor, trg_tensor)\n",
    "    \n",
    "        best_guess = output.argmax(2)[:, -1].item()\n",
    "        outputs.append(best_guess)\n",
    "    \n",
    "        if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "    \n",
    "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
    "    # remove start token\n",
    "    return translated_sentence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1622732281392,
     "user": {
      "displayName": "Yizhou Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKweum2fN1Zce6MyyPbghO4Je2ulM6lfRBIfn7=s64",
      "userId": "03363274278730990809"
     },
     "user_tz": -120
    },
    "id": "XAIG7WjtY5iE"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_model=False\n",
    "save_model=True\n",
    "\n",
    "num_epochs=20\n",
    "learning_rate=1e-4\n",
    "batch_size=32\n",
    "\n",
    "src_vocab_size=len(german.vocab)\n",
    "trg_vocab_size=len(english.vocab)\n",
    "embedding_size=512\n",
    "num_heads=8\n",
    "num_encoder_layers=3\n",
    "num_decoder_layers=3\n",
    "dropout=0.10\n",
    "max_len=100\n",
    "\n",
    "forward_expansion=4\n",
    "src_pad_idx=english.vocab.stoi[\"<pad>\"]\n",
    "writer=SummaryWriter(\"runs/loss_plot\")\n",
    "step=0\n",
    "\n",
    "train_iterator,valid_iterator,test_iterator=BucketIterator.splits(\n",
    "    (train_data,valid_data,test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 621831,
     "status": "ok",
     "timestamp": 1622732909894,
     "user": {
      "displayName": "Yizhou Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiKweum2fN1Zce6MyyPbghO4Je2ulM6lfRBIfn7=s64",
      "userId": "03363274278730990809"
     },
     "user_tz": -120
    },
    "id": "bbWOt3XDY5iE",
    "outputId": "a813d16b-8ffa-4d6e-de2d-443b574201bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch1/20]\n",
      "Translated example sentence: \n",
      " ['doorway', 'shielding', 'parasail', 'comforts', 'marked', 'ad', 'cup', 'shepherd', 'headband', 'geometric', 'interacting', 'stockings', 'preschool', 'hikers', 'metropolitan', 'pen', 'took', 'makeup', 'klein', 'shoe', 'dries', 'swimming', 'vegetation', 'multitasking', 'boys', 'rescue', 'spoon', 'structures', 'who', 'converses', 'better', 'seattle', 'shovel', 'hidden', 'driver', 'dark', 'mock', 'tambourine', 'hazard', 'they', 'object', 'league', 'pacifier', 'sections', 'upwards', 'blank', 'claus', 'whisking', 'cigarette', 'cakes', 'member', 'squat', 'participates', 'attempts', 'raises', 'famous', 'sandy', 'nature', 'feeding', 'lots', 'suspenders', 'teammates', 'foods', 'licking', 'gentlemen', 'necktie', 'couples', 'insect', 'electronics', 'science', 'idle', 'provide', 'created', 'picks', 'beads', 'removing', 'camps', 'butchering', 'clapping', 'tilts', 'workman', 'end', 'snowbank', 'artists', 'elephant', 'these', 'rugby', 'sundress', 'giant', 'ribbons', 'beneath', 'projection', 'attendant', 'gambling', 'couch', 'instructors', 'sequence', 'expressions', 'instructor', 'cash']\n",
      "[Epoch2/20]\n",
      "Translated example sentence: \n",
      " ['a', 'person', 'is', 'walking', 'down', 'a', 'large', 'bike', '.', '<eos>']\n",
      "[Epoch3/20]\n",
      "Translated example sentence: \n",
      " ['a', 'black', 'walking', 'down', 'a', 'building', 'is', 'walking', 'by', 'a', 'building', '.', '<eos>']\n",
      "[Epoch4/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'walks', 'through', 'a', 'bridge', 'near', 'a', 'mountain', '.', '<eos>']\n",
      "[Epoch5/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'walks', 'by', 'a', 'bridge', 'near', 'a', 'boat', '.', '<eos>']\n",
      "[Epoch6/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'walks', 'by', 'a', 'bridge', 'next', 'to', 'a', 'bridge', '.', '<eos>']\n",
      "[Epoch7/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'walks', 'by', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '<eos>']\n",
      "[Epoch8/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'walking', 'under', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '<eos>']\n",
      "[Epoch9/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'walks', 'under', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '<eos>']\n",
      "[Epoch10/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'walks', 'under', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '<eos>']\n",
      "[Epoch11/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'walks', 'beside', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '<eos>']\n",
      "[Epoch12/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'is', 'walking', 'beside', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '<eos>']\n",
      "[Epoch13/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'walks', 'under', 'a', 'bridge', 'next', 'to', 'a', 'bridge', '.', '<eos>']\n",
      "[Epoch14/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'walks', 'under', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '<eos>']\n",
      "[Epoch15/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'is', 'walking', 'beside', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '<eos>']\n",
      "[Epoch16/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'walks', 'beside', 'a', 'bridge', 'near', 'a', 'bridge', '.', '<eos>']\n",
      "[Epoch17/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'is', 'walking', 'under', 'a', 'bridge', 'next', 'to', 'a', 'boat', '.', '<eos>']\n",
      "[Epoch18/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'is', 'walking', 'beside', 'a', 'bridge', 'next', 'to', 'a', 'bridge', '.', '<eos>']\n",
      "[Epoch19/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'walks', 'by', 'a', 'bridge', 'near', 'a', 'boat', '.', '<eos>']\n",
      "[Epoch20/20]\n",
      "Translated example sentence: \n",
      " ['a', 'horse', 'is', 'walking', 'beside', 'a', 'bridge', 'near', 'a', 'boat', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "model=Transformer(\n",
    "    embedding_size,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device\n",
    "    ).to(device)\n",
    "optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "pad_idx=english.vocab.stoi[\"<pad>\"]\n",
    "criterion=nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "sentence=\"ein pferd geht unter einer brücke neben einem boot durch\"\n",
    "\n",
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.ptar\"),model.optimzier)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch{epoch+1}/{num_epochs}]\")\n",
    "    model.eval()\n",
    "    translated_sentence=translate_sentence(\n",
    "        model,sentence,german,english,device,max_length=100\n",
    "    )\n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx,batch in enumerate(train_iterator):\n",
    "        inp_data=batch.src.transpose(0,1).to(device)\n",
    "        target=batch.trg.transpose(0,1).to(device)\n",
    "        \n",
    "        output=model(inp_data,target[:,:-1])\n",
    "        \n",
    "        output=output.reshape(-1,output.shape[2])\n",
    "        target=target[:,1:].reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss=criterion(output,target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1)\n",
    "        optimizer.step()\n",
    "        #writer.add_scalar(\"training loss\",loss,global_step=step)\n",
    "        step+=1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
